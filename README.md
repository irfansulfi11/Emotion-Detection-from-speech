
# ğŸ™ï¸ Emotion Detection from Speech using Deep Learning

This project detects human emotions from audio speech using a deep learning model. It leverages spectrogram analysis and a hybrid CNN-RNN architecture (SEDNet) to classify emotions accurately from voice signals.

## ğŸš€ Overview

The goal is to automatically recognize emotions from speech audio samples. The system processes `.wav` files to extract Mel spectrograms, then passes them through a deep learning model that classifies the emotions into categories like *happy*, *sad*, *angry*, etc.

## ğŸ“ Dataset

**Toronto Emotional Speech Set (TESS)**  
- Contains recordings of 7 emotions: **Angry, Disgust, Fear, Happy, Neutral, Pleasant Surprise, Sad**
- Each audio file is a `.wav` file
- Dataset structure:
  ```
  dataset/
  â”œâ”€â”€ Angry/
  â”œâ”€â”€ Disgust/
  â”œâ”€â”€ Fear/
  â”œâ”€â”€ Happy/
  â”œâ”€â”€ Neutral/
  â”œâ”€â”€ Pleasant_Surprise/
  â””â”€â”€ Sad/
  ```

## ğŸ§  Model Architecture

**SEDNet (Spectrogram-based Emotion Detection Network)**  
- **CNN layers**: For spatial feature extraction from spectrograms  
- **RNN layers (LSTM/GRU)**: For capturing temporal features  
- **Fully Connected Layers**: For final emotion classification  
- **Activation**: Softmax

## ğŸ”§ Technologies Used

- Python
- TensorFlow / Keras
- Librosa (for audio processing)
- NumPy, Matplotlib
- Scikit-learn (for evaluation metrics)
- OpenCV (optional, for visualization)

## ğŸ“Š Training & Results

- **Input**: Mel Spectrograms (128 Mel bands)
- **Optimizer**: Adam
- **Loss Function**: Categorical Crossentropy
- **Accuracy Achieved**: ~93% on test data
- **Augmentations Used**:
  - Time Stretching
  - Pitch Shifting
  - Background Noise Addition

## ğŸ“ˆ Evaluation Metrics

- Accuracy
- Confusion Matrix
- Precision, Recall, F1-score

## ğŸ’¡ Key Features

- End-to-end audio-based emotion recognition
- Robust against variations in speaker tone and speed
- Can be extended to real-time speech emotion recognition

## ğŸ› ï¸ Setup Instructions

```bash
# Clone the repo
git clone https://github.com/yourusername/emotion-detection-from-speech.git
cd emotion-detection-from-speech

# Install dependencies
pip install -r requirements.txt

# Run training
python train.py

# Evaluate
python evaluate.py
```

## ğŸ§ª Demo

You can test the model by placing a `.wav` file in the `/test_audio` directory and running:

```bash
python predict.py --file test_audio/sample.wav
```

## ğŸ“Œ Future Work

- Integrate with real-time speech input (microphone)
- Deploy as a Flask web app or Streamlit GUI
- Support for multilingual emotion detection

## ğŸ“š References

- [TESS Dataset](https://dataverse.scholarsportal.info/dataset.xhtml?persistentId=doi:10.5683/SP2/E8H2MF)
- Papers on SEDNet and Speech Emotion Recognition
- Librosa and TensorFlow documentation
